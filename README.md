# MCP-Markdown-RAG

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-green)](LICENSE)
[![MCP Server](https://img.shields.io/badge/MCP-Server-blue)](https://modelcontextprotocol.io)
[![Python](https://img.shields.io/badge/Python-%3E%3D3.10-blue.svg)](https://python.org/)

A semantic search engine for your markdown documents. An MCP server that indexes notes, docs, and knowledge bases into a Milvus vector database, letting AI assistants find relevant content by **meaning**.

> Ask *"what are the tradeoffs of microservices?"* and find your notes about service boundaries, distributed systems, and API design â€” even if none of them mention "microservices."

## Features

- **Semantic matching** â€” finds conceptually related content, not just keyword hits
- **Multi-provider embeddings** â€” Gemini, OpenAI, Vertex AI, Voyage, or local models
- **Smart incremental indexing** â€” mtime/size fast-path skips unchanged files without reading them; hash only computed when metadata changes
- **Single-pass delta scan** â€” detects new, changed, and deleted files in one directory walk
- **Stale vector pruning** â€” automatically removes vectors for deleted or moved files from Milvus
- **Batch embedding** â€” concurrent batches with rate-limit retry (429 exponential backoff)
- **Batch insert** â€” chunked Milvus inserts to stay under the gRPC 64MB message limit
- **Shell reindex CLI** â€” `reindex.py` for large-scale indexing with real-time progress logs
- **Configurable exclusions** â€” skip directories (`node_modules`, `.git`, `_legacy`) and files (`AGENTS.md`) via env
- **Milvus Standalone support** â€” connect to a Docker-based Milvus server for multi-agent concurrent access
- **MCP native** â€” works with any MCP host (Claude Code, Cursor, Windsurf, VS Code, Antigravity, Codex, etc.)

## Architecture

```mermaid
graph TB
    subgraph MCP["MCP Server (server.py)"]
        direction TB
        IDX["index_documents<br/>Incremental Indexing"]
        SEARCH["search_documents<br/>Semantic Search"]
        CLEAR["clear_index<br/>Reset"]
    end

    subgraph Indexing["Indexing Engine (utils.py)"]
        DELTA["get_index_delta<br/>Single-pass Delta Scan"]
        TRACK["index_tracking.json<br/>mtime / size / hash"]
        CHUNK["llama-index<br/>SentenceSplitter"]
    end

    subgraph Embed["Embedding Providers"]
        VERTEX["Vertex AI<br/>gemini-embedding-001"]
        GEMINI["Gemini API<br/>OpenAI-compat"]
        OAI["OpenAI / Compatible"]
        VOYAGE["Voyage AI<br/>voyage-3"]
        LOCAL["Milvus Built-in<br/>DefaultEmbeddingFunction"]
    end

    subgraph Store["Vector Store"]
        MILVUS["Milvus Standalone<br/>Docker (gRPC)"]
        LITE["Milvus Lite<br/>SQLite (local)"]
    end

    IDX --> DELTA --> CHUNK --> Embed --> Store
    SEARCH --> Embed --> Store
    DELTA <--> TRACK

    style MCP fill:#2d3748,color:#e2e8f0
    style Embed fill:#553c9a,color:#e9d8fd
    style Store fill:#2a4365,color:#bee3f8
    style Indexing fill:#22543d,color:#c6f6d5
```

## How It Works

```mermaid
flowchart LR
    A["ğŸ“ Markdown Files"] -->|"directory walk\n+ exclude filter"| B["ğŸ” Delta Scan\nmtime/size check"]
    B -->|changed| C["âœ‚ï¸ Chunk\nSentenceSplitter"]
    B -->|unchanged| SKIP["â­ï¸ Skip"]
    B -->|deleted| PRUNE["ğŸ—‘ï¸ Prune\nMilvus delete"]
    C --> D["ğŸ§  Embed\nVertex/Gemini/OpenAI"]
    D -->|"batch insert"| E["ğŸ’¾ Milvus\nVector Store"]

    F["ğŸ” Search Query"] --> D
    D -->|"cosine similarity"| G["ğŸ“Š Top-K Results\nwith relevance %"]

    style A fill:#2d3748,color:#e2e8f0
    style D fill:#553c9a,color:#e9d8fd
    style E fill:#2a4365,color:#bee3f8
    style G fill:#22543d,color:#c6f6d5
    style PRUNE fill:#742a2a,color:#fed7d7
```

## Quick Start

Requires [uv](https://docs.astral.sh/uv/) (Python package manager).

### 1. Clone

```bash
git clone https://github.com/bitkyc08-arch/mcp-markdown-rag.git
```

### 2. Configure

Add to your MCP host config:

```json
{
  "mcpServers": {
    "markdown-rag": {
      "command": "uv",
      "args": [
        "--directory", "/path/to/mcp-markdown-rag",
        "run", "server.py"
      ],
      "env": {
        "EMBEDDING_PROVIDER": "gemini",
        "EMBEDDING_MODEL": "gemini-embedding-001",
        "EMBEDDING_DIM": "768",
        "GEMINI_API_KEY": "${GEMINI_API_KEY}",
        "MILVUS_ADDRESS": "http://localhost:19530"
      }
    }
  }
}
```

> **Tip**: For local-only use (no Docker), omit `MILVUS_ADDRESS` â€” it defaults to a local SQLite-based Milvus Lite file (`.db/milvus_markdown.db`).

## Embedding Providers

| Provider              | `EMBEDDING_PROVIDER` | Default Model            | Auth            |
| --------------------- | -------------------- | ------------------------ | --------------- |
| **Vertex AI**         | `vertex`             | `gemini-embedding-001`   | Service Account |
| **Gemini**            | `gemini`             | `gemini-embedding-001`   | API key         |
| **OpenAI**            | `openai`             | `text-embedding-3-small` | API key         |
| **OpenAI-compatible** | `openai-compatible`  | `text-embedding-3-small` | API key         |
| **Voyage**            | `voyage`             | `voyage-3`               | API key         |
| **Local**             | `local`              | Milvus built-in (768d)   | â€”               |

<details>
<summary><strong>Vertex AI</strong> â€” Google Cloud í”„ë¡œë•ì…˜ ê¶Œì¥</summary>

Google Cloudì˜ Vertex AIë¥¼ í†µí•´ `gemini-embedding-001` ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. API key ëŒ€ì‹  **Service Account ì¸ì¦**ì„ ì‚¬ìš©í•˜ë©°, OAuth í† í°ì´ ìë™ ê°±ì‹ ë©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤.

**ì¥ì **: ë†’ì€ Rate Limit, ìë™ í† í° ê°±ì‹ , GCP í”„ë¡œì íŠ¸ ë‹¨ìœ„ ë¹Œë§
**ë‹¨ì **: GCP í”„ë¡œì íŠ¸ + Service Account ì„¤ì • í•„ìš”

**ì‚¬ì „ ì¤€ë¹„**:
1. GCP í”„ë¡œì íŠ¸ ìƒì„± & Vertex AI API í™œì„±í™”
2. Service Account ìƒì„± â†’ JSON í‚¤ ë‹¤ìš´ë¡œë“œ
3. `Vertex AI User` ì—­í•  ë¶€ì—¬

```json
{
  "EMBEDDING_PROVIDER": "vertex",
  "EMBEDDING_MODEL": "gemini-embedding-001",
  "EMBEDDING_DIM": "768",
  "GOOGLE_APPLICATION_CREDENTIALS": "/path/to/service-account.json",
  "VERTEX_PROJECT": "your-gcp-project-id",
  "VERTEX_LOCATION": "us-central1"
}
```

**ì°¸ê³ **: `VERTEX_LOCATION`ì€ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ë¦¬ì „ì— ë§ì¶°ì•¼ í•©ë‹ˆë‹¤. `gemini-embedding-001`ì€ `us-central1`ì—ì„œ ì‚¬ìš© ê°€ëŠ¥. ì „ì²´ ë¦¬ì „ ëª©ë¡ì€ [Vertex AI ë¬¸ì„œ](https://cloud.google.com/vertex-ai/docs/general/locations)ë¥¼ ì°¸ê³ .

</details>

<details>
<summary><strong>Gemini</strong> â€” ë¹ ë¥¸ ì‹œì‘ì— ê°€ì¥ ì‰¬ì›€</summary>

Google AI Studioì˜ Gemini APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. API key í•˜ë‚˜ë©´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•´ì„œ ê°€ì¥ ê°„ë‹¨í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ OpenAI-compatible ì—”ë“œí¬ì¸íŠ¸(`generativelanguage.googleapis.com/v1beta/openai/`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**ì¥ì **: ê°€ì… í›„ ì¦‰ì‹œ ì‚¬ìš©, ë¬´ë£Œ Tier ìˆìŒ
**ë‹¨ì **: Rate Limitì´ Vertex ëŒ€ë¹„ ë‚®ìŒ (ë¶„ë‹¹ 1,500 RPM ê¸°ë³¸)

**ì‚¬ì „ ì¤€ë¹„**:
1. [Google AI Studio](https://aistudio.google.com/)ì—ì„œ API key ë°œê¸‰

```json
{
  "EMBEDDING_PROVIDER": "gemini",
  "EMBEDDING_MODEL": "gemini-embedding-001",
  "EMBEDDING_DIM": "768",
  "GEMINI_API_KEY": "your-api-key"
}
```

**ì°¸ê³ **: ëŒ€ëŸ‰ ì¸ë±ì‹±(1000+ íŒŒì¼) ì‹œ 429 ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `EMBEDDING_BATCH_DELAY_MS=1000`ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì•ˆì •ì ì…ë‹ˆë‹¤.

</details>

<details>
<summary><strong>OpenAI</strong> â€” text-embedding-3 ì‹œë¦¬ì¦ˆ</summary>

OpenAIì˜ ì„ë² ë”© APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `text-embedding-3-small` (1536d)ê³¼ `text-embedding-3-large` (3072d) ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤. `EMBEDDING_DIM`ìœ¼ë¡œ ì°¨ì›ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Matryoshka representation).

**ì¥ì **: ë†’ì€ í’ˆì§ˆ, ì°¨ì› ì¶•ì†Œ ì§€ì›
**ë‹¨ì **: ìœ ë£Œ (small: $0.02/1M tokens, large: $0.13/1M tokens)

**ì‚¬ì „ ì¤€ë¹„**:
1. [OpenAI Platform](https://platform.openai.com/)ì—ì„œ API key ë°œê¸‰

```json
{
  "EMBEDDING_PROVIDER": "openai",
  "EMBEDDING_MODEL": "text-embedding-3-small",
  "EMBEDDING_DIM": "768",
  "OPENAI_API_KEY": "sk-..."
}
```

**ì°¸ê³ **: `EMBEDDING_DIM`ì„ 768ë¡œ ì„¤ì •í•˜ë©´ ì›ë˜ 1536d ë²¡í„°ë¥¼ 768dë¡œ ì¤„ì—¬ì„œ ì €ì¥í•©ë‹ˆë‹¤. ê²€ìƒ‰ í’ˆì§ˆì€ ì†Œí­ ê°ì†Œí•˜ì§€ë§Œ ìŠ¤í† ë¦¬ì§€ì™€ ì†ë„ê°€ ê°œì„ ë©ë‹ˆë‹¤.

</details>

<details>
<summary><strong>OpenAI-compatible</strong> â€” ìì²´ í˜¸ìŠ¤íŒ… / ì¨ë“œíŒŒí‹° API</summary>

OpenAI API í˜•ì‹ì„ ë”°ë¥´ëŠ” ëª¨ë“  ì„ë² ë”© ì„œë¹„ìŠ¤ì— ì—°ê²°í•©ë‹ˆë‹¤. Ollama, LM Studio, Azure OpenAI, Together AI, Fireworks AI ë“± ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ì™€ í˜¸í™˜ë©ë‹ˆë‹¤.

**ì¥ì **: ìì²´ í˜¸ìŠ¤íŒ… ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥, í”„ë¼ì´ë²„ì‹œ ë³´ì¥
**ë‹¨ì **: ì„œë¹„ìŠ¤ë³„ ì„¤ì •ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

```json
{
  "EMBEDDING_PROVIDER": "openai-compatible",
  "EMBEDDING_MODEL": "nomic-embed-text",
  "EMBEDDING_DIM": "768",
  "EMBEDDING_API_KEY": "your-api-key-or-dummy",
  "EMBEDDING_BASE_URL": "http://localhost:11434/v1"
}
```

**Ollama ì˜ˆì‹œ**: Ollamaì—ì„œ `nomic-embed-text`ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:

```bash
ollama pull nomic-embed-text
# EMBEDDING_BASE_URL=http://localhost:11434/v1
# EMBEDDING_API_KEY=ollama  (ì•„ë¬´ ê°’ì´ë‚˜ OK)
```

**Azure OpenAI ì˜ˆì‹œ**:

```json
{
  "EMBEDDING_BASE_URL": "https://your-resource.openai.azure.com/openai/deployments/your-deployment",
  "EMBEDDING_API_KEY": "your-azure-api-key"
}
```

</details>

<details>
<summary><strong>Voyage</strong> â€” Retrieval íŠ¹í™” ì„ë² ë”©</summary>

Voyage AIì˜ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. `voyage-3`ì€ ê²€ìƒ‰(retrieval) íƒœìŠ¤í¬ì— ìµœì í™”ë˜ì–´ ìˆì–´ì„œ RAGì— íŠ¹íˆ ì í•©í•©ë‹ˆë‹¤. Anthropicì´ Claudeì— ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© providerë¡œë„ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.

**ì¥ì **: RAG/ê²€ìƒ‰ í’ˆì§ˆ ìµœìƒìœ„ê¶Œ, ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì› (ìµœëŒ€ 32K tokens)
**ë‹¨ì **: ìœ ë£Œ ($0.06/1M tokens), ë¬´ë£Œ Tier ì œí•œì 

**ì‚¬ì „ ì¤€ë¹„**:
1. [Voyage AI](https://www.voyageai.com/)ì—ì„œ API key ë°œê¸‰

```json
{
  "EMBEDDING_PROVIDER": "voyage",
  "EMBEDDING_MODEL": "voyage-3",
  "VOYAGE_API_KEY": "pa-..."
}
```

**ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸**:

| ëª¨ë¸            | ì°¨ì› | ìµœëŒ€ í† í° | ìš©ë„        |
| --------------- | ---- | --------- | ----------- |
| `voyage-3`      | 1024 | 32K       | ë²”ìš© (ê¶Œì¥) |
| `voyage-3-lite` | 512  | 32K       | ê²½ëŸ‰/ì €ë¹„ìš© |
| `voyage-code-3` | 1024 | 32K       | ì½”ë“œ íŠ¹í™”   |

**ì°¸ê³ **: `EMBEDDING_DIM`ì„ ë³„ë„ ì„¤ì •í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤. VoyageëŠ” ëª¨ë¸ë³„ ê³ ì • ì°¨ì›ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

</details>

<details>
<summary><strong>Local</strong> â€” ì˜¤í”„ë¼ì¸ / ë¬´ë£Œ</summary>

Milvusì— ë‚´ì¥ëœ ê¸°ë³¸ ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (`DefaultEmbeddingFunction`, 768d). ì¸í„°ë„· ì—°ê²°ì´ë‚˜ API key ì—†ì´ ì™„ì „í•œ ë¡œì»¬ í™˜ê²½ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤.

**ì¥ì **: ë¬´ë£Œ, ì˜¤í”„ë¼ì¸ ì‚¬ìš©, API ì˜ì¡´ì„± ì—†ìŒ
**ë‹¨ì **: í´ë¼ìš°ë“œ ëª¨ë¸ ëŒ€ë¹„ ê²€ìƒ‰ í’ˆì§ˆ ë‚®ìŒ, ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ ì†Œìš”

```json
{
  "EMBEDDING_PROVIDER": "local"
}
```

ë³„ë„ í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤. `EMBEDDING_PROVIDER`ë¥¼ ìƒëµí•´ë„ ê¸°ë³¸ê°’ì´ `local`ì…ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë‚˜ í”„ë¡œí† íƒ€ì´í•‘ì— ì í•©í•©ë‹ˆë‹¤.

</details>

## Tools

| Tool               | Description                                                                                                                             |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------- |
| `index_documents`  | Index markdown files with incremental updates. Automatically detects new, changed, and deleted files. Prunes stale vectors from Milvus. |
| `search_documents` | Semantic search across indexed documents. Returns top-k results with relevance scores and file paths.                                   |
| `clear_index`      | Reset the vector database and tracking state.                                                                                           |

## Incremental Indexing & Pruning

The indexing engine uses a **single-pass delta scan** to efficiently detect what changed:

```mermaid
flowchart TD
    START["Directory Walk"] --> NEW{"New file?"}
    NEW -->|yes| INDEX["âœ… Index"]
    NEW -->|no| META{"mtime/size same?"}
    META -->|yes| SKIP["â­ï¸ Skip\n(no file read)"]
    META -->|no| HASH{"Compute hash\nContent changed?"}
    HASH -->|yes| REINDEX["ğŸ”„ Re-index"]
    HASH -->|no| UPDATE["ğŸ“ Update tracking\n(metadata only)"]
    START --> MISSING{"Tracked but\nmissing from disk?"}
    MISSING -->|yes| PRUNE["ğŸ—‘ï¸ Prune from Milvus\n+ remove tracking"]

    style INDEX fill:#22543d,color:#c6f6d5
    style SKIP fill:#2d3748,color:#e2e8f0
    style REINDEX fill:#744210,color:#fefcbf
    style PRUNE fill:#742a2a,color:#fed7d7
```

**Performance** (1300+ files, 1 file changed):

| Metric                              | Result                              |
| ----------------------------------- | ----------------------------------- |
| Unchanged files â€” hash computations | **0** (mtime/size fast-path)        |
| Changed file â€” embed + insert       | **~3 seconds**                      |
| No changes â€” full scan              | **instant** ("Already up to date!") |
| Deleted file â€” prune + scan         | **instant**                         |

### How pruning works

When a file is deleted or moved to an excluded directory (e.g. `_legacy/`), the next incremental `index_documents` call will:

1. Detect the file is missing from the current directory scan
2. Delete its vectors from Milvus (`filter: path == '...'`)
3. Remove it from the tracking file
4. Return a message: `"Pruned N deleted/moved files."`

No manual cleanup needed â€” just delete the file and re-index.

## Shell Reindex CLI

For large-scale indexing (1000+ files), use `reindex.py` directly for real-time logs and better error handling:

```bash
cd /path/to/mcp-markdown-rag

# Incremental (changed files only)
EMBEDDING_PROVIDER=vertex \
MILVUS_ADDRESS=http://localhost:19530 \
GOOGLE_APPLICATION_CREDENTIALS=/path/to/sa.json \
VERTEX_PROJECT=your-project-id \
VERTEX_LOCATION=us-central1 \
uv run python reindex.py /path/to/vault

# Full rebuild (drop + re-create collection)
uv run python reindex.py /path/to/vault --force
```

Features over MCP `index_documents`:
- Real-time progress logs (batch N/M, elapsed time)
- 429 rate-limit retry with exponential backoff (5 attempts)
- Chunked Milvus insert (configurable via `MILVUS_INSERT_BATCH`)
- Non-recursive mode (`--no-recursive`)

## Configuration

### Core

| Variable             | Default                  | Description                                                          |
| -------------------- | ------------------------ | -------------------------------------------------------------------- |
| `EMBEDDING_PROVIDER` | `local`                  | `gemini`, `openai`, `openai-compatible`, `vertex`, `voyage`, `local` |
| `EMBEDDING_MODEL`    | (provider default)       | Model name override                                                  |
| `EMBEDDING_DIM`      | `768`                    | Vector dimension                                                     |
| `MILVUS_ADDRESS`     | `.db/milvus_markdown.db` | Milvus address (`http://host:port`) or local file path               |

### Indexing Tuning

| Variable                       | Default | Description                                                                |
| ------------------------------ | ------- | -------------------------------------------------------------------------- |
| `MARKDOWN_CHUNK_SIZE`          | `2048`  | Token chunk size for splitting documents                                   |
| `MARKDOWN_CHUNK_OVERLAP`       | `100`   | Token overlap between chunks                                               |
| `EMBEDDING_BATCH_SIZE`         | `250`   | Texts per embedding API call                                               |
| `EMBEDDING_BATCH_DELAY_MS`     | `0`     | Delay between embedding batches (ms). Set to `1000` for rate-limited APIs. |
| `EMBEDDING_CONCURRENT_BATCHES` | `4`     | Parallel embedding batches                                                 |
| `MILVUS_INSERT_BATCH`          | `5000`  | Rows per Milvus insert call (gRPC 64MB limit)                              |

### Exclusions

| Variable                 | Default | Description                                                                                                                                    |
| ------------------------ | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `MARKDOWN_EXCLUDE_DIRS`  | â€”       | Extra directories to exclude (comma-separated). Added to built-in: `node_modules`, `__pycache__`, `devlog`, `_legacy`, `dist`, `build`, `.git` |
| `MARKDOWN_EXCLUDE_FILES` | â€”       | Extra files to exclude (comma-separated). Added to built-in: `AGENTS.md`, `CLAUDE.md`, `GEMINI.md`                                             |

### Provider Auth

| Variable                         | Description                                 |
| -------------------------------- | ------------------------------------------- |
| `GEMINI_API_KEY`                 | Gemini API key                              |
| `OPENAI_API_KEY`                 | OpenAI API key                              |
| `VOYAGE_API_KEY`                 | Voyage API key                              |
| `EMBEDDING_API_KEY`              | OpenAI-compatible API key                   |
| `EMBEDDING_BASE_URL`             | OpenAI-compatible base URL                  |
| `GOOGLE_APPLICATION_CREDENTIALS` | Service account JSON path for Vertex AI     |
| `VERTEX_PROJECT`                 | GCP project ID (auto-detected if SA has it) |
| `VERTEX_LOCATION`                | Vertex AI region (default: `us-central1`)   |

### Vertex AI Example

```json
{
  "mcpServers": {
    "markdown-rag": {
      "command": "uv",
      "args": ["--directory", "/path/to/mcp-markdown-rag", "run", "server.py"],
      "env": {
        "EMBEDDING_PROVIDER": "vertex",
        "EMBEDDING_MODEL": "gemini-embedding-001",
        "EMBEDDING_DIM": "768",
        "MARKDOWN_CHUNK_SIZE": "2048",
        "MARKDOWN_CHUNK_OVERLAP": "120",
        "EMBEDDING_BATCH_SIZE": "100",
        "EMBEDDING_BATCH_DELAY_MS": "1000",
        "EMBEDDING_CONCURRENT_BATCHES": "3",
        "MILVUS_INSERT_BATCH": "5000",
        "MILVUS_ADDRESS": "http://localhost:19530",
        "GOOGLE_APPLICATION_CREDENTIALS": "/path/to/service-account.json",
        "VERTEX_PROJECT": "your-gcp-project-id",
        "VERTEX_LOCATION": "us-central1"
      }
    }
  }
}
```

## Debugging

```bash
npx @modelcontextprotocol/inspector uv --directory /path/to/mcp-markdown-rag run server.py
```

## License

Apache License 2.0 â€” see [LICENSE](LICENSE).

---

### About

This project is a fork of [MCP-Markdown-RAG](https://github.com/Zackriya-Solutions/MCP-Markdown-RAG) by Zackriya Solutions, heavily extended for production use.

**Key additions over upstream**:
- Multi-provider embeddings (Vertex AI, Gemini, OpenAI, Voyage)
- Single-pass incremental indexing with mtime/size fast-path
- Stale vector pruning for deleted/moved files
- Batch embedding with 429 retry + batch Milvus insert (gRPC 64MB limit)
- Shell reindex CLI (`reindex.py`) with real-time progress
- Configurable file/directory exclusions
- Milvus Standalone (Docker) support for multi-agent concurrent access
- Search results with relevance scores and file paths
