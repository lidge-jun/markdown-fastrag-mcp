# MCP-Markdown-RAG

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-green)](LICENSE)
[![MCP Server](https://img.shields.io/badge/MCP-Server-blue)](https://modelcontextprotocol.io)
[![Python](https://img.shields.io/badge/Python-%3E%3D3.10-blue.svg)](https://python.org/)

A semantic search engine for your markdown documents. An MCP server that indexes notes, docs, and knowledge bases into a Milvus vector database, letting AI assistants find relevant content by **meaning**.

> Ask *"what are the tradeoffs of microservices?"* and find your notes about service boundaries, distributed systems, and API design â€” even if none of them mention "microservices."

## Features

- **Semantic matching** â€” finds conceptually related content, not just keyword hits
- **Multi-provider embeddings** â€” Gemini, OpenAI, Vertex AI, Voyage, or local models
- **Smart incremental indexing** â€” mtime/size fast-path skips unchanged files without reading them; hash only computed when metadata changes
- **Single-pass delta scan** â€” detects new, changed, and deleted files in one directory walk
- **Stale vector pruning** â€” automatically removes vectors for deleted or moved files from Milvus
- **Batch embedding** â€” concurrent batches with rate-limit retry (429 exponential backoff)
- **Batch insert** â€” chunked Milvus inserts to stay under the gRPC 64MB message limit
- **Shell reindex CLI** â€” `reindex.py` for large-scale indexing with real-time progress logs
- **Configurable exclusions** â€” skip directories (`node_modules`, `.git`, `_legacy`) and files (`AGENTS.md`) via env
- **Milvus Standalone support** â€” connect to a Docker-based Milvus server for multi-agent concurrent access
- **MCP native** â€” works with any MCP host (Claude Code, Cursor, Windsurf, VS Code, Antigravity, Codex, etc.)

## Architecture

```mermaid
graph TB
    subgraph MCP["MCP Server (server.py)"]
        direction TB
        IDX["index_documents<br/>Incremental Indexing"]
        SEARCH["search_documents<br/>Semantic Search"]
        CLEAR["clear_index<br/>Reset"]
    end

    subgraph Indexing["Indexing Engine (utils.py)"]
        DELTA["get_index_delta<br/>Single-pass Delta Scan"]
        TRACK["index_tracking.json<br/>mtime / size / hash"]
        CHUNK["llama-index<br/>SentenceSplitter"]
    end

    subgraph Embed["Embedding Providers"]
        VERTEX["Vertex AI<br/>gemini-embedding-001"]
        GEMINI["Gemini API<br/>OpenAI-compat"]
        OAI["OpenAI / Compatible"]
        VOYAGE["Voyage AI<br/>voyage-3"]
        LOCAL["Milvus Built-in<br/>DefaultEmbeddingFunction"]
    end

    subgraph Store["Vector Store"]
        MILVUS["Milvus Standalone<br/>Docker (gRPC)"]
        LITE["Milvus Lite<br/>SQLite (local)"]
    end

    IDX --> DELTA --> CHUNK --> Embed --> Store
    SEARCH --> Embed --> Store
    DELTA <--> TRACK

    style MCP fill:#2d3748,color:#e2e8f0
    style Embed fill:#553c9a,color:#e9d8fd
    style Store fill:#2a4365,color:#bee3f8
    style Indexing fill:#22543d,color:#c6f6d5
```

## How It Works

```mermaid
flowchart LR
    A["ğŸ“ Markdown Files"] -->|"directory walk\n+ exclude filter"| B["ğŸ” Delta Scan\nmtime/size check"]
    B -->|changed| C["âœ‚ï¸ Chunk\nSentenceSplitter"]
    B -->|unchanged| SKIP["â­ï¸ Skip"]
    B -->|deleted| PRUNE["ğŸ—‘ï¸ Prune\nMilvus delete"]
    C --> D["ğŸ§  Embed\nVertex/Gemini/OpenAI"]
    D -->|"batch insert"| E["ğŸ’¾ Milvus\nVector Store"]

    F["ğŸ” Search Query"] --> D
    D -->|"cosine similarity"| G["ğŸ“Š Top-K Results\nwith relevance %"]

    style A fill:#2d3748,color:#e2e8f0
    style D fill:#553c9a,color:#e9d8fd
    style E fill:#2a4365,color:#bee3f8
    style G fill:#22543d,color:#c6f6d5
    style PRUNE fill:#742a2a,color:#fed7d7
```

## Quick Start

Requires [uv](https://docs.astral.sh/uv/) (Python package manager).

### 1. Clone

```bash
git clone https://github.com/bitkyc08-arch/mcp-markdown-rag.git
```

### 2. Configure

Add to your MCP host config:

```json
{
  "mcpServers": {
    "markdown-rag": {
      "command": "uv",
      "args": [
        "--directory", "/path/to/mcp-markdown-rag",
        "run", "server.py"
      ],
      "env": {
        "EMBEDDING_PROVIDER": "gemini",
        "EMBEDDING_MODEL": "gemini-embedding-001",
        "EMBEDDING_DIM": "768",
        "GEMINI_API_KEY": "${GEMINI_API_KEY}",
        "MILVUS_ADDRESS": "http://localhost:19530"
      }
    }
  }
}
```

> **Tip**: For local-only use (no Docker), omit `MILVUS_ADDRESS` â€” it defaults to a local SQLite-based Milvus Lite file (`.db/milvus_markdown.db`).

## Embedding Providers

| Provider              | `EMBEDDING_PROVIDER` | Default Model            | Auth            |
| --------------------- | -------------------- | ------------------------ | --------------- |
| **Vertex AI**         | `vertex`             | `gemini-embedding-001`   | Service Account |
| **Gemini**            | `gemini`             | `gemini-embedding-001`   | API key         |
| **OpenAI**            | `openai`             | `text-embedding-3-small` | API key         |
| **OpenAI-compatible** | `openai-compatible`  | `text-embedding-3-small` | API key         |
| **Voyage**            | `voyage`             | `voyage-3`               | API key         |
| **Local**             | `local`              | Milvus built-in (768d)   | â€”               |

<details>
<summary><strong>Vertex AI</strong> â€” Google Cloud í”„ë¡œë•ì…˜ ê¶Œì¥</summary>

Google Cloudì˜ Vertex AIë¥¼ í†µí•´ `gemini-embedding-001` ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. API key ëŒ€ì‹  **Service Account ì¸ì¦**ì„ ì‚¬ìš©í•˜ë©°, OAuth í† í°ì´ ìë™ ê°±ì‹ ë©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤.

**ì¥ì **: ë†’ì€ Rate Limit, ìë™ í† í° ê°±ì‹ , GCP í”„ë¡œì íŠ¸ ë‹¨ìœ„ ë¹Œë§
**ë‹¨ì **: GCP í”„ë¡œì íŠ¸ + Service Account ì„¤ì • í•„ìš”

**ì‚¬ì „ ì¤€ë¹„**:
1. GCP í”„ë¡œì íŠ¸ ìƒì„± & Vertex AI API í™œì„±í™”
2. Service Account ìƒì„± â†’ JSON í‚¤ ë‹¤ìš´ë¡œë“œ
3. `Vertex AI User` ì—­í•  ë¶€ì—¬

```json
{
  "EMBEDDING_PROVIDER": "vertex",
  "EMBEDDING_MODEL": "gemini-embedding-001",
  "EMBEDDING_DIM": "768",
  "GOOGLE_APPLICATION_CREDENTIALS": "/path/to/service-account.json",
  "VERTEX_PROJECT": "your-gcp-project-id",
  "VERTEX_LOCATION": "us-central1"
}
```

**ì°¸ê³ **: `VERTEX_LOCATION`ì€ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ë¦¬ì „ì— ë§ì¶°ì•¼ í•©ë‹ˆë‹¤. `gemini-embedding-001`ì€ `us-central1`ì—ì„œ ì‚¬ìš© ê°€ëŠ¥. ì „ì²´ ë¦¬ì „ ëª©ë¡ì€ [Vertex AI ë¬¸ì„œ](https://cloud.google.com/vertex-ai/docs/general/locations)ë¥¼ ì°¸ê³ .

</details>

<details>
<summary><strong>Gemini</strong> â€” ë¹ ë¥¸ ì‹œì‘ì— ê°€ì¥ ì‰¬ì›€</summary>

Google AI Studioì˜ Gemini APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. API key í•˜ë‚˜ë©´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•´ì„œ ê°€ì¥ ê°„ë‹¨í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ OpenAI-compatible ì—”ë“œí¬ì¸íŠ¸(`generativelanguage.googleapis.com/v1beta/openai/`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**ì¥ì **: ê°€ì… í›„ ì¦‰ì‹œ ì‚¬ìš©, ë¬´ë£Œ Tier ìˆìŒ
**ë‹¨ì **: Rate Limitì´ Vertex ëŒ€ë¹„ ë‚®ìŒ (ë¶„ë‹¹ 1,500 RPM ê¸°ë³¸)

**ì‚¬ì „ ì¤€ë¹„**:
1. [Google AI Studio](https://aistudio.google.com/)ì—ì„œ API key ë°œê¸‰

```json
{
  "EMBEDDING_PROVIDER": "gemini",
  "EMBEDDING_MODEL": "gemini-embedding-001",
  "EMBEDDING_DIM": "768",
  "GEMINI_API_KEY": "your-api-key"
}
```

**ì°¸ê³ **: ëŒ€ëŸ‰ ì¸ë±ì‹±(1000+ íŒŒì¼) ì‹œ 429 ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `EMBEDDING_BATCH_DELAY_MS=1000`ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì•ˆì •ì ì…ë‹ˆë‹¤.

</details>

<details>
<summary><strong>OpenAI</strong> â€” text-embedding-3 ì‹œë¦¬ì¦ˆ</summary>

OpenAIì˜ ì„ë² ë”© APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `text-embedding-3-small` (1536d)ê³¼ `text-embedding-3-large` (3072d) ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤. `EMBEDDING_DIM`ìœ¼ë¡œ ì°¨ì›ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Matryoshka representation).

**ì¥ì **: ë†’ì€ í’ˆì§ˆ, ì°¨ì› ì¶•ì†Œ ì§€ì›
**ë‹¨ì **: ìœ ë£Œ (small: $0.02/1M tokens, large: $0.13/1M tokens)

**ì‚¬ì „ ì¤€ë¹„**:
1. [OpenAI Platform](https://platform.openai.com/)ì—ì„œ API key ë°œê¸‰

```json
{
  "EMBEDDING_PROVIDER": "openai",
  "EMBEDDING_MODEL": "text-embedding-3-small",
  "EMBEDDING_DIM": "768",
  "OPENAI_API_KEY": "sk-..."
}
```

**ì°¸ê³ **: `EMBEDDING_DIM`ì„ 768ë¡œ ì„¤ì •í•˜ë©´ ì›ë˜ 1536d ë²¡í„°ë¥¼ 768dë¡œ ì¤„ì—¬ì„œ ì €ì¥í•©ë‹ˆë‹¤. ê²€ìƒ‰ í’ˆì§ˆì€ ì†Œí­ ê°ì†Œí•˜ì§€ë§Œ ìŠ¤í† ë¦¬ì§€ì™€ ì†ë„ê°€ ê°œì„ ë©ë‹ˆë‹¤.

</details>

<details>
<summary><strong>OpenAI-compatible</strong> â€” ìì²´ í˜¸ìŠ¤íŒ… / ì¨ë“œíŒŒí‹° API</summary>

OpenAI API í˜•ì‹ì„ ë”°ë¥´ëŠ” ëª¨ë“  ì„ë² ë”© ì„œë¹„ìŠ¤ì— ì—°ê²°í•©ë‹ˆë‹¤. Ollama, LM Studio, Azure OpenAI, Together AI, Fireworks AI ë“± ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ì™€ í˜¸í™˜ë©ë‹ˆë‹¤.

**ì¥ì **: ìì²´ í˜¸ìŠ¤íŒ… ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥, í”„ë¼ì´ë²„ì‹œ ë³´ì¥
**ë‹¨ì **: ì„œë¹„ìŠ¤ë³„ ì„¤ì •ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

```json
{
  "EMBEDDING_PROVIDER": "openai-compatible",
  "EMBEDDING_MODEL": "nomic-embed-text",
  "EMBEDDING_DIM": "768",
  "EMBEDDING_API_KEY": "your-api-key-or-dummy",
  "EMBEDDING_BASE_URL": "http://localhost:11434/v1"
}
```

**Ollama ì˜ˆì‹œ**: Ollamaì—ì„œ `nomic-embed-text`ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:

```bash
ollama pull nomic-embed-text
# EMBEDDING_BASE_URL=http://localhost:11434/v1
# EMBEDDING_API_KEY=ollama  (ì•„ë¬´ ê°’ì´ë‚˜ OK)
```

**Azure OpenAI ì˜ˆì‹œ**:

```json
{
  "EMBEDDING_BASE_URL": "https://your-resource.openai.azure.com/openai/deployments/your-deployment",
  "EMBEDDING_API_KEY": "your-azure-api-key"
}
```

</details>

<details>
<summary><strong>Voyage</strong> â€” Retrieval íŠ¹í™” ì„ë² ë”©</summary>

Voyage AIì˜ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. `voyage-3`ì€ ê²€ìƒ‰(retrieval) íƒœìŠ¤í¬ì— ìµœì í™”ë˜ì–´ ìˆì–´ì„œ RAGì— íŠ¹íˆ ì í•©í•©ë‹ˆë‹¤. Anthropicì´ Claudeì— ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© providerë¡œë„ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.

**ì¥ì **: RAG/ê²€ìƒ‰ í’ˆì§ˆ ìµœìƒìœ„ê¶Œ, ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì› (ìµœëŒ€ 32K tokens)
**ë‹¨ì **: ìœ ë£Œ ($0.06/1M tokens), ë¬´ë£Œ Tier ì œí•œì 

**ì‚¬ì „ ì¤€ë¹„**:
1. [Voyage AI](https://www.voyageai.com/)ì—ì„œ API key ë°œê¸‰

```json
{
  "EMBEDDING_PROVIDER": "voyage",
  "EMBEDDING_MODEL": "voyage-3",
  "VOYAGE_API_KEY": "pa-..."
}
```

**ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸**:

| ëª¨ë¸            | ì°¨ì› | ìµœëŒ€ í† í° | ìš©ë„        |
| --------------- | ---- | --------- | ----------- |
| `voyage-3`      | 1024 | 32K       | ë²”ìš© (ê¶Œì¥) |
| `voyage-3-lite` | 512  | 32K       | ê²½ëŸ‰/ì €ë¹„ìš© |
| `voyage-code-3` | 1024 | 32K       | ì½”ë“œ íŠ¹í™”   |

**ì°¸ê³ **: `EMBEDDING_DIM`ì„ ë³„ë„ ì„¤ì •í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤. VoyageëŠ” ëª¨ë¸ë³„ ê³ ì • ì°¨ì›ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

</details>

<details>
<summary><strong>Local</strong> â€” ì˜¤í”„ë¼ì¸ / ë¬´ë£Œ</summary>

Milvusì— ë‚´ì¥ëœ ê¸°ë³¸ ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (`DefaultEmbeddingFunction`, 768d). ì¸í„°ë„· ì—°ê²°ì´ë‚˜ API key ì—†ì´ ì™„ì „í•œ ë¡œì»¬ í™˜ê²½ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤.

**ì¥ì **: ë¬´ë£Œ, ì˜¤í”„ë¼ì¸ ì‚¬ìš©, API ì˜ì¡´ì„± ì—†ìŒ
**ë‹¨ì **: í´ë¼ìš°ë“œ ëª¨ë¸ ëŒ€ë¹„ ê²€ìƒ‰ í’ˆì§ˆ ë‚®ìŒ, ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ ì†Œìš”

```json
{
  "EMBEDDING_PROVIDER": "local"
}
```

ë³„ë„ í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤. `EMBEDDING_PROVIDER`ë¥¼ ìƒëµí•´ë„ ê¸°ë³¸ê°’ì´ `local`ì…ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë‚˜ í”„ë¡œí† íƒ€ì´í•‘ì— ì í•©í•©ë‹ˆë‹¤.

</details>

## Tools

| Tool               | Description                                                                                                                             |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------- |
| `index_documents`  | Index markdown files with incremental updates. Automatically detects new, changed, and deleted files. Prunes stale vectors from Milvus. |
| `search_documents` | Semantic search across indexed documents. Returns top-k results with relevance scores and file paths.                                   |
| `clear_index`      | Reset the vector database and tracking state.                                                                                           |

## Incremental Indexing & Pruning

The indexing engine uses a **single-pass delta scan** (`get_index_delta()`) to efficiently detect new, changed, and deleted files in one directory walk â€” no separate passes needed.

```mermaid
flowchart TD
    START["Directory Walk<br/>(single pass)"] --> NEW{"New file?<br/>(not in tracking)"}
    NEW -->|yes| INDEX["âœ… Index<br/>chunk â†’ embed â†’ insert"]
    NEW -->|no| META{"mtime + size<br/>same as tracked?"}
    META -->|"yes (fast-path)"| SKIP["â­ï¸ Skip<br/>no file read, no hash<br/>zero I/O cost"]
    META -->|no| HASH{"Read file â†’ MD5<br/>hash changed?"}
    HASH -->|yes| REINDEX["ğŸ”„ Re-index<br/>delete old vectors â†’ re-embed"]
    HASH -->|"no (e.g. touch)"| UPDATE["ğŸ“ Update tracking<br/>refresh mtime/size only"]
    START --> MISSING{"Tracked file<br/>missing from disk?"}
    MISSING -->|yes| PRUNE["ğŸ—‘ï¸ Prune<br/>delete vectors from Milvus<br/>+ remove from tracking"]

    style INDEX fill:#22543d,color:#c6f6d5
    style SKIP fill:#2d3748,color:#e2e8f0
    style REINDEX fill:#744210,color:#fefcbf
    style PRUNE fill:#742a2a,color:#fed7d7
    style META fill:#553c9a,color:#e9d8fd
```

### Optimization Techniques

<details>
<summary><strong>1. mtime/size Fast-Path</strong> â€” íŒŒì¼ì„ ì½ì§€ ì•Šê³  ë³€ê²½ ì—¬ë¶€ íŒë‹¨</summary>

ì „í†µì ì¸ ì¦ë¶„ ì¸ë±ì‹±ì€ ëª¨ë“  íŒŒì¼ì„ ì—´ì–´ì„œ í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. 1300ê°œ íŒŒì¼ì´ë©´ 1300ë²ˆ íŒŒì¼ I/Oê°€ ë°œìƒí•©ë‹ˆë‹¤.

ì´ ì„œë²„ëŠ” `os.stat()` ì‹œìŠ¤í…œì½œë¡œ **mtime(ìˆ˜ì • ì‹œê°)ê³¼ size(íŒŒì¼ í¬ê¸°)ë§Œ ë¨¼ì € í™•ì¸**í•©ë‹ˆë‹¤. ì´ ë‘ ê°’ì´ trackingê³¼ ë™ì¼í•˜ë©´ ë‚´ìš©ì´ ë°”ë€Œì§€ ì•Šì•˜ë‹¤ê³  íŒë‹¨í•˜ê³  **íŒŒì¼ì„ ì•„ì˜ˆ ì—´ì§€ ì•ŠìŠµë‹ˆë‹¤**.

```python
# Fast path: íŒŒì¼ì„ ì½ì§€ ì•Šê³  ë©”íƒ€ë°ì´í„°ë§Œ ë¹„êµ
file_stat = os.stat(file_path)  # ì‹œìŠ¤í…œì½œ 1íšŒ (ns ë‹¨ìœ„)
if stored_mtime == file_stat.st_mtime and stored_size == file_stat.st_size:
    continue  # íŒŒì¼ ì½ê¸° 0íšŒ, í•´ì‹œ ê³„ì‚° 0íšŒ
```

**íš¨ê³¼**: 1300ê°œ íŒŒì¼ ìŠ¤ìº” ì‹œ í•´ì‹œ ê³„ì‚° 0íšŒ â†’ ì „ì²´ ìŠ¤ìº”ì´ ìˆ˜ ë°€ë¦¬ì´ˆì— ì™„ë£Œ.

</details>

<details>
<summary><strong>2. Single-Pass Delta Scan</strong> â€” ë³€ê²½ + ì‚­ì œë¥¼ í•œ ë²ˆì— ê°ì§€</summary>

ê¸°ì¡´ êµ¬í˜„ì€ ë‘ ë²ˆì˜ ìŠ¤ìº”ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤:
- Pass 1: `get_changed_files()` â€” ë””ë ‰í† ë¦¬ë¥¼ ìˆœíšŒí•˜ë©° ë³€ê²½ëœ íŒŒì¼ ì°¾ê¸°
- Pass 2: `get_deleted_files()` â€” tracking ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©° ì‚­ì œëœ íŒŒì¼ ì°¾ê¸°

ì´ê²ƒì„ `get_index_delta()`ë¡œ í†µí•©í•˜ì—¬ **í•œ ë²ˆì˜ ë””ë ‰í† ë¦¬ ìˆœíšŒ**ë¡œ ë³€ê²½ê³¼ ì‚­ì œë¥¼ ë™ì‹œì— ê°ì§€í•©ë‹ˆë‹¤.

```python
def get_index_delta(directory, recursive=False) -> tuple[list[str], list[str]]:
    md_files = list_md_files(directory, recursive)
    current_files_set = set(md_files)

    # Pass 1 of 1: ì‚­ì œ ê°ì§€ (trackingì— ìˆì§€ë§Œ ë””ìŠ¤í¬ì— ì—†ëŠ” íŒŒì¼)
    for tracked_path in list(tracking_data.keys()):
        if tracked_path not in current_files_set:
            deleted_files.append(tracked_path)

    # Pass 1 of 1 (ê³„ì†): ë³€ê²½ ê°ì§€ (mtime/size fast-path â†’ hash fallback)
    for file_path in md_files:
        # ... mtime/size ë¹„êµ â†’ hash ë¹„êµ
    
    return changed_files, deleted_files  # í•œ ë²ˆì— ë°˜í™˜
```

**íš¨ê³¼**: 2-pass â†’ 1-passë¡œ ë””ë ‰í† ë¦¬ ìˆœíšŒ íšŸìˆ˜ ì ˆë°˜. ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ **2.28x ì†ë„ ê°œì„ **.

</details>

<details>
<summary><strong>3. Tracking Format í™•ì¥</strong> â€” í•˜ìœ„í˜¸í™˜ ìœ ì§€í•˜ë©´ì„œ size ì¶”ê°€</summary>

tracking íŒŒì¼ (`index_tracking.json`)ì˜ í¬ë§·ì„ í™•ì¥í•˜ì—¬ íŒŒì¼ í¬ê¸° ì •ë³´ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.

```
Before: [hash, mtime]            â† ê¸°ì¡´ í¬ë§·
After:  [hash, mtime, size]      â† í™•ì¥ í¬ë§·
```

`_parse_tracking_entry()` íŒŒì„œê°€ ë‘ í¬ë§· ëª¨ë‘ ì½ì„ ìˆ˜ ìˆì–´ì„œ **ê¸°ì¡´ ë°ì´í„°ë¥¼ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤**. ë‹¤ìŒ ìŠ¤ìº” ì‹œ ìë™ìœ¼ë¡œ sizeê°€ ì¶”ê°€ë©ë‹ˆë‹¤.

</details>

<details>
<summary><strong>4. Batch Embedding + Rate Limit Retry</strong> â€” ëŒ€ëŸ‰ ì„ë² ë”© ì•ˆì •ì„±</summary>

ì„ë² ë”© APIì— ìˆ˜ì²œ ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ ë³´ë‚¼ ë•Œ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤:

| ë¬¸ì œ                        | í•´ê²°                                                        |
| --------------------------- | ----------------------------------------------------------- |
| API 429 (Too Many Requests) | Exponential backoff ì¬ì‹œë„ (5s â†’ 10s â†’ 20s â†’ 40s, ìµœëŒ€ 5íšŒ) |
| gRPC 64MB ë©”ì‹œì§€ ì´ˆê³¼       | `MILVUS_INSERT_BATCH=5000`ìœ¼ë¡œ ë¶„í•  insert                  |
| ëŒ€ëŸ‰ ìš”ì²­ ì‹œ ë©”ëª¨ë¦¬         | `EMBEDDING_BATCH_SIZE=100`ìœ¼ë¡œ ë§ˆì´í¬ë¡œ ë°°ì¹˜                |
| API ê°„ ë”œë ˆì´               | `EMBEDDING_BATCH_DELAY_MS=1000`ìœ¼ë¡œ ì¡°ì ˆ                    |

</details>

### Performance

| Metric                                | Result                              |
| ------------------------------------- | ----------------------------------- |
| Unchanged files â€” hash computations   | **0** (mtime/size fast-path)        |
| Changed file â€” embed + insert         | **~3 seconds**                      |
| No changes â€” full scan                | **instant** ("Already up to date!") |
| Deleted file â€” prune + scan           | **instant**                         |
| Full reindex (1300 files, 23K chunks) | **~7â€“8 minutes**                    |
| 1-pass vs 2-pass scan time            | **2.28x faster**                    |

### How Pruning Works

When a file is deleted or moved to an excluded directory (e.g. `_legacy/`), the next incremental `index_documents` call will:

1. Detect the file is missing from the current directory scan
2. Delete its vectors from Milvus (`filter: path == '...'`)
3. Remove it from the tracking file
4. Return a message: `"Pruned N deleted/moved files."`

No manual cleanup needed â€” just delete the file and re-index.

## Shell Reindex CLI

### MCP vs Shell â€” ì–¸ì œ ë¬´ì—‡ì„ ì“°ë‚˜?

| ìƒí™©                             | MCP `index_documents` | Shell `reindex.py` |
| -------------------------------- | :-------------------: | :----------------: |
| íŒŒì¼ ëª‡ ê°œ ë³€ê²½ í›„ ì¦ë¶„ ì—…ë°ì´íŠ¸ |           âœ…           |                    |
| 1000+ íŒŒì¼ ì „ì²´ ì¬ì¸ë±ì‹±         |                       |         âœ…          |
| ëª¨ë…¸ë ˆí¬ / ëŒ€ê·œëª¨ ì½”ë“œë² ì´ìŠ¤     |                       |         âœ…          |
| 429/gRPC ì—ëŸ¬ ë””ë²„ê¹… í•„ìš”        |                       |         âœ…          |
| ì‹¤ì‹œê°„ ì§„í–‰ë¥  ë¡œê·¸ í™•ì¸          |                       |         âœ…          |
| AI ì—ì´ì „íŠ¸ê°€ ìë™ìœ¼ë¡œ ì‹¤í–‰      |           âœ…           |                    |

MCP ë„êµ¬(`index_documents`)ëŠ” **ìµœì¢… ê²°ê³¼ë§Œ ë°˜í™˜**í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì‹œê°„ ë¡œê·¸ë¥¼ ë³¼ ìˆ˜ ì—†ê³ , ëŒ€ëŸ‰ ì¸ë±ì‹± ì‹œ íƒ€ì„ì•„ì›ƒ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. `reindex.py`ëŠ” shellì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ë°°ì¹˜ ì§„í–‰ë¥ , ì—ëŸ¬, ì†Œìš” ì‹œê°„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Usage

```bash
cd /path/to/mcp-markdown-rag

# Incremental (changed + deleted files only)
EMBEDDING_PROVIDER=vertex \
MILVUS_ADDRESS=http://localhost:19530 \
GOOGLE_APPLICATION_CREDENTIALS=/path/to/sa.json \
VERTEX_PROJECT=your-project-id \
VERTEX_LOCATION=us-central1 \
uv run python reindex.py /path/to/vault

# Full rebuild (âš ï¸ drops and re-creates collection)
uv run python reindex.py /path/to/vault --force

# Non-recursive (top-level files only)
uv run python reindex.py /path/to/vault --no-recursive
```

### Shell Reindex Features

| Feature             | Description                                                            |
| ------------------- | ---------------------------------------------------------------------- |
| Real-time progress  | `Batch 72/119 â€” elapsed 4m32s`                                         |
| 429 retry           | Exponential backoff (5s, 10s, 20s, 40s, 80s) Ã— 5 attempts              |
| Insert batching     | `MILVUS_INSERT_BATCH=5000` chunks per gRPC call                        |
| Incremental + prune | Same `get_index_delta()` as MCP â€” detects changes AND deletions        |
| `--force`           | Drop collection â†’ full re-embed (use for schema changes or corruption) |

<details>
<summary><strong>Mono-repo / Large Codebase Guide</strong></summary>

ëª¨ë…¸ë ˆí¬ë‚˜ ëŒ€ê·œëª¨ ë¬¸ì„œ ë³¼íŠ¸(1000+ íŒŒì¼)ì—ì„œëŠ” MCP ëŒ€ì‹  shell reindexë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

**ì²« ì¸ë±ì‹± (ì „ì²´)**:
```bash
# ì „ì²´ ë³¼íŠ¸ë¥¼ í•œ ë²ˆì— ì¸ë±ì‹± (ì•½ 7-8ë¶„)
uv run python reindex.py /path/to/monorepo --force
```

**ì´í›„ ì¼ìƒ ì—…ë°ì´íŠ¸**:
```bash
# ë³€ê²½ëœ íŒŒì¼ë§Œ ì¦ë¶„ ì¸ë±ì‹± (ìˆ˜ ì´ˆ)
uv run python reindex.py /path/to/monorepo
```

**ë””ë ‰í† ë¦¬ ì œì™¸**:
```bash
# íŠ¹ì • ë””ë ‰í† ë¦¬ ì œì™¸ (envë¡œ ì„¤ì •)
MARKDOWN_EXCLUDE_DIRS="_legacy,archive,vendor" \
uv run python reindex.py /path/to/monorepo
```

**Rate limit ë³´ìˆ˜ì  ì„¤ì •** (Vertex AI ë¬´ë£Œ Tier ë“±):
```bash
EMBEDDING_BATCH_SIZE=50 \
EMBEDDING_BATCH_DELAY_MS=2000 \
EMBEDDING_CONCURRENT_BATCHES=2 \
uv run python reindex.py /path/to/vault --force
```

</details>

<details>
<summary><strong>RAG Skill Reference</strong> â€” AI ì—ì´ì „íŠ¸ìš© ê¶Œì¥ ì›Œí¬í”Œë¡œìš°</summary>

AI ì—ì´ì „íŠ¸(Claude Code, Antigravity, Codex ë“±)ê°€ ì´ ì„œë²„ë¥¼ ì‚¬ìš©í•  ë•Œì˜ ê¶Œì¥ ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤. `.agents/skills/rag/SKILL.md`ì—ì„œ ë°œì·Œ:

**Document RAG Flow**:
```
index_documents(directory, recursive=true) â†’ search_documents(query, k)
```

**When to use Shell vs MCP**:
- MCP: ì†Œê·œëª¨ ì¦ë¶„ ì—…ë°ì´íŠ¸ (ì¼ìƒì  ì‚¬ìš©)
- Shell: ì „ì²´ ë¦¬ì¸ë±ì‹± (`--force`), 1000+ íŒŒì¼ ëŒ€ëŸ‰ ì—…ë°ì´íŠ¸, ì—ëŸ¬ ë””ë²„ê¹…

**Query Language Policy**:
- Code RAG â†’ English queries
- Document RAG â†’ User's language (e.g. Korean)

**Destructive Operations (ì£¼ì˜)**:
- `index_documents(force_reindex=true)` â€” ì»¬ë ‰ì…˜ drop í›„ ì¬ìƒì„±
- `clear_index` â€” ì „ì²´ ë²¡í„° + tracking ì‚­ì œ
- ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ë•Œë§Œ ì‹¤í–‰

</details>

## Configuration

### Core

| Variable             | Default                  | Description                                                          |
| -------------------- | ------------------------ | -------------------------------------------------------------------- |
| `EMBEDDING_PROVIDER` | `local`                  | `gemini`, `openai`, `openai-compatible`, `vertex`, `voyage`, `local` |
| `EMBEDDING_MODEL`    | (provider default)       | Model name override                                                  |
| `EMBEDDING_DIM`      | `768`                    | Vector dimension                                                     |
| `MILVUS_ADDRESS`     | `.db/milvus_markdown.db` | Milvus address (`http://host:port`) or local file path               |

### Indexing Tuning

| Variable                       | Default | Description                                                                |
| ------------------------------ | ------- | -------------------------------------------------------------------------- |
| `MARKDOWN_CHUNK_SIZE`          | `2048`  | Token chunk size for splitting documents                                   |
| `MARKDOWN_CHUNK_OVERLAP`       | `100`   | Token overlap between chunks                                               |
| `EMBEDDING_BATCH_SIZE`         | `250`   | Texts per embedding API call                                               |
| `EMBEDDING_BATCH_DELAY_MS`     | `0`     | Delay between embedding batches (ms). Set to `1000` for rate-limited APIs. |
| `EMBEDDING_CONCURRENT_BATCHES` | `4`     | Parallel embedding batches                                                 |
| `MILVUS_INSERT_BATCH`          | `5000`  | Rows per Milvus insert call (gRPC 64MB limit)                              |

### Exclusions

| Variable                 | Default | Description                                                                                                                                    |
| ------------------------ | ------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `MARKDOWN_EXCLUDE_DIRS`  | â€”       | Extra directories to exclude (comma-separated). Added to built-in: `node_modules`, `__pycache__`, `devlog`, `_legacy`, `dist`, `build`, `.git` |
| `MARKDOWN_EXCLUDE_FILES` | â€”       | Extra files to exclude (comma-separated). Added to built-in: `AGENTS.md`, `CLAUDE.md`, `GEMINI.md`                                             |

### Provider Auth

| Variable                         | Description                                 |
| -------------------------------- | ------------------------------------------- |
| `GEMINI_API_KEY`                 | Gemini API key                              |
| `OPENAI_API_KEY`                 | OpenAI API key                              |
| `VOYAGE_API_KEY`                 | Voyage API key                              |
| `EMBEDDING_API_KEY`              | OpenAI-compatible API key                   |
| `EMBEDDING_BASE_URL`             | OpenAI-compatible base URL                  |
| `GOOGLE_APPLICATION_CREDENTIALS` | Service account JSON path for Vertex AI     |
| `VERTEX_PROJECT`                 | GCP project ID (auto-detected if SA has it) |
| `VERTEX_LOCATION`                | Vertex AI region (default: `us-central1`)   |

### Vertex AI Example

```json
{
  "mcpServers": {
    "markdown-rag": {
      "command": "uv",
      "args": ["--directory", "/path/to/mcp-markdown-rag", "run", "server.py"],
      "env": {
        "EMBEDDING_PROVIDER": "vertex",
        "EMBEDDING_MODEL": "gemini-embedding-001",
        "EMBEDDING_DIM": "768",
        "MARKDOWN_CHUNK_SIZE": "2048",
        "MARKDOWN_CHUNK_OVERLAP": "120",
        "EMBEDDING_BATCH_SIZE": "100",
        "EMBEDDING_BATCH_DELAY_MS": "1000",
        "EMBEDDING_CONCURRENT_BATCHES": "3",
        "MILVUS_INSERT_BATCH": "5000",
        "MILVUS_ADDRESS": "http://localhost:19530",
        "GOOGLE_APPLICATION_CREDENTIALS": "/path/to/service-account.json",
        "VERTEX_PROJECT": "your-gcp-project-id",
        "VERTEX_LOCATION": "us-central1"
      }
    }
  }
}
```

## Debugging

```bash
npx @modelcontextprotocol/inspector uv --directory /path/to/mcp-markdown-rag run server.py
```

## License

Apache License 2.0 â€” see [LICENSE](LICENSE).

---

### About

This project is a fork of [MCP-Markdown-RAG](https://github.com/Zackriya-Solutions/MCP-Markdown-RAG) by Zackriya Solutions, heavily extended for production use.

**Key additions over upstream**:
- Multi-provider embeddings (Vertex AI, Gemini, OpenAI, Voyage)
- Single-pass incremental indexing with mtime/size fast-path
- Stale vector pruning for deleted/moved files
- Batch embedding with 429 retry + batch Milvus insert (gRPC 64MB limit)
- Shell reindex CLI (`reindex.py`) with real-time progress
- Configurable file/directory exclusions
- Milvus Standalone (Docker) support for multi-agent concurrent access
- Search results with relevance scores and file paths
