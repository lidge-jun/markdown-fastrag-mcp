#!/usr/bin/env python3
"""
markdown-rag ë¦¬ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ (shell ì§ì ‘ ì‹¤í–‰ìš©)
MCP ì„œë²„ ì—†ì´ ì§ì ‘ Milvusì— ì¸ë±ì‹±, ë¡œê·¸ ì¶œë ¥

í™˜ê²½ ë³€ìˆ˜ëŠ” MCP configì™€ ë™ì¼í•˜ê²Œ ì„¤ì • í•„ìš”:
  EMBEDDING_PROVIDER, EMBEDDING_MODEL, EMBEDDING_DIM,
  MILVUS_ADDRESS, GOOGLE_APPLICATION_CREDENTIALS ë“±
"""
import asyncio
import os
import sys
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_DIR)

# env ê¸°ë³¸ê°’ (server.pyì™€ ë™ì¼ â€” ì—†ìœ¼ë©´ server.py ê¸°ë³¸ê°’ ì‚¬ìš©)
os.environ.setdefault("EMBEDDING_BATCH_SIZE", "100")
os.environ.setdefault("EMBEDDING_CONCURRENT_BATCHES", "3")
os.environ.setdefault("EMBEDDING_BATCH_DELAY_MS", "1000")

# === ì„í¬íŠ¸ (server.pyê°€ env ê¸°ë°˜ìœ¼ë¡œ provider ìë™ ì„ íƒ) ===
from server import (
    embedding_fn, milvus_client, COLLECTION_NAME,
    MARKDOWN_CHUNK_SIZE, MARKDOWN_CHUNK_OVERLAP,
    EMBEDDING_BATCH_SIZE, EMBEDDING_CONCURRENT_BATCHES,
    EMBEDDING_BATCH_DELAY_MS, EMBEDDING_PROVIDER,
    MIN_CHUNK_TOKENS,
)
from utils import ensure_collection, get_index_delta, list_md_files, update_tracking_file
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import MarkdownNodeParser
from llama_index.core.text_splitter import TokenTextSplitter
from concurrent.futures import ThreadPoolExecutor

# Milvus gRPC 64MB ì œí•œ ëŒ€ë¹„ insert ë°°ì¹˜ í¬ê¸°
MILVUS_INSERT_BATCH = int(os.getenv("MILVUS_INSERT_BATCH", "5000"))


def log(msg: str):
    ts = time.strftime("%H:%M:%S")
    print(f"[{ts}] {msg}", flush=True)


async def reindex(target_path: str, recursive: bool = True, force: bool = False):
    log(f"ğŸš€ Reindex ì‹œì‘: {target_path}")
    log(f"   recursive={recursive}, force={force}")
    log(f"   Milvus: {os.getenv('MILVUS_ADDRESS', 'local')}")
    log(f"   Embedding: {EMBEDDING_PROVIDER}")
    log(f"   chunk_size={MARKDOWN_CHUNK_SIZE}, overlap={MARKDOWN_CHUNK_OVERLAP}")
    log(f"   batch_size={EMBEDDING_BATCH_SIZE}, concurrent={EMBEDDING_CONCURRENT_BATCHES}")
    log(f"   insert_batch={MILVUS_INSERT_BATCH}")
    print()

    # 1. íŒŒì¼ ìˆ˜ì§‘
    if force:
        log("ğŸ—‘ï¸  Force mode: ê¸°ì¡´ ì»¬ë ‰ì…˜ ë“œë¡­ í›„ ì¬ìƒì„±...")
        if milvus_client.has_collection(COLLECTION_NAME):
            milvus_client.drop_collection(COLLECTION_NAME)
        ensure_collection(milvus_client)

        all_files = list_md_files(target_path, recursive=recursive)
        log(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ {len(all_files)}ê°œ ë°œê²¬")
        if not all_files:
            log("âŒ ì¸ë±ì‹±í•  íŒŒì¼ ì—†ìŒ!")
            return

        documents = SimpleDirectoryReader(
            input_files=all_files, required_exts=[".md"]
        ).load_data()
        processed_files = [doc.metadata["file_path"] for doc in documents]
    else:
        ensure_collection(milvus_client)

        # Single-pass delta scan (changed + deleted) for faster incremental indexing.
        changed_files, deleted_files = get_index_delta(target_path, recursive=recursive)
        pruned_count = 0
        for file_path in deleted_files:
            try:
                milvus_client.delete(
                    collection_name=COLLECTION_NAME, filter=f"path == '{file_path}'"
                )
                pruned_count += 1
            except Exception:
                continue
        if pruned_count > 0:
            log(f"ğŸ—‘ï¸  Pruned {pruned_count} deleted/moved files from index")

        if not changed_files:
            if pruned_count > 0:
                log(f"âœ… Pruned {pruned_count} stale files. No new changes.")
            else:
                log("âœ… Already up to date!")
            return
        log(f"ğŸ“„ ë³€ê²½ëœ íŒŒì¼ {len(changed_files)}ê°œ:")
        for f in changed_files[:20]:
            log(f"   {os.path.basename(f)}")
        if len(changed_files) > 20:
            log(f"   ... ì™¸ {len(changed_files) - 20}ê°œ")

        for file_path in changed_files:
            try:
                milvus_client.delete(
                    collection_name=COLLECTION_NAME, filter=f"path == '{file_path}'"
                )
            except Exception:
                continue

        documents = SimpleDirectoryReader(
            input_files=changed_files, required_exts=[".md"]
        ).load_data()
        processed_files = changed_files

    # 2. Pre-process: strip YAML frontmatter before chunking
    from chunking import (
        _normalize_meta,
        inject_header_prefix,
        merge_small_chunks,
        strip_frontmatter,
    )

    for doc in documents:
        clean_text, fm = strip_frontmatter(doc.text)
        doc.text = clean_text
        doc.metadata["tags"] = _normalize_meta(fm.get("tags"))
        doc.metadata["aliases"] = _normalize_meta(fm.get("aliases"))

    # 3. ì²­í‚¹
    log(f"âœ‚ï¸  ì²­í‚¹ ì¤‘... (documents={len(documents)})")
    nodes = MarkdownNodeParser(chunk_size=MARKDOWN_CHUNK_SIZE).get_nodes_from_documents(documents)
    chunk_overlap = min(MARKDOWN_CHUNK_OVERLAP, max(0, MARKDOWN_CHUNK_SIZE - 1))
    chunked_nodes = TokenTextSplitter(
        chunk_size=MARKDOWN_CHUNK_SIZE, chunk_overlap=chunk_overlap
    ).get_nodes_from_documents(nodes)
    chunked_nodes = [node for node in chunked_nodes if node.text.strip()]

    # Post-process: merge small chunks + inject parent header context.
    if MIN_CHUNK_TOKENS > 0:
        pre_merge = len(chunked_nodes)
        chunked_nodes = merge_small_chunks(
            chunked_nodes, MIN_CHUNK_TOKENS, MARKDOWN_CHUNK_SIZE
        )
        if pre_merge != len(chunked_nodes):
            log(f"   â†’ ë³‘í•©: {pre_merge} â†’ {len(chunked_nodes)} ì²­í¬")
    chunked_nodes = inject_header_prefix(chunked_nodes)
    log(f"   â†’ {len(chunked_nodes)} ì²­í¬ ìƒì„± (min_tokens={MIN_CHUNK_TOKENS})")

    # 3. ì„ë² ë”©
    texts = [node.text for node in chunked_nodes]
    batches = [texts[i:i + EMBEDDING_BATCH_SIZE] for i in range(0, len(texts), EMBEDDING_BATCH_SIZE)]
    total_batches = len(batches)
    log(f"ğŸ§  ì„ë² ë”© ì‹œì‘: {len(texts)} ì²­í¬, {total_batches} ë°°ì¹˜")

    vectors = [None] * len(texts)
    executor = ThreadPoolExecutor(max_workers=EMBEDDING_CONCURRENT_BATCHES)
    loop = asyncio.get_event_loop()
    t0 = time.time()

    async def embed_one(batch_idx, batch, offset):
        log(f"   ë°°ì¹˜ {batch_idx+1}/{total_batches} ({offset}~{offset+len(batch)}/{len(texts)})")
        for retry in range(5):
            try:
                result = await loop.run_in_executor(executor, embedding_fn.encode_documents, batch)
                for j, vec in enumerate(result):
                    vectors[offset + j] = vec
                return
            except Exception as e:
                if "429" in str(e) and retry < 4:
                    wait = (2 ** retry) * 5  # 5s, 10s, 20s, 40s
                    log(f"   âš ï¸  429 Rate Limit! {wait}s ëŒ€ê¸° í›„ ì¬ì‹œë„ ({retry+1}/5)")
                    await asyncio.sleep(wait)
                else:
                    raise

    for wave_start in range(0, total_batches, EMBEDDING_CONCURRENT_BATCHES):
        wave = []
        for k in range(wave_start, min(wave_start + EMBEDDING_CONCURRENT_BATCHES, total_batches)):
            offset = k * EMBEDDING_BATCH_SIZE
            wave.append(embed_one(k, batches[k], offset))
        await asyncio.gather(*wave)
        if EMBEDDING_BATCH_DELAY_MS > 0:
            await asyncio.sleep(EMBEDDING_BATCH_DELAY_MS / 1000.0)

    executor.shutdown(wait=False)
    elapsed = time.time() - t0
    log(f"   â†’ ì„ë² ë”© ì™„ë£Œ ({elapsed:.1f}s)")

    # 4. Milvus ì‚½ì… (gRPC 64MB ì œí•œ ëŒ€ë¹„ ë°°ì¹˜ ì²˜ë¦¬)
    data = [
        {
            "vector": vector,
            "text": node.text,
            "filename": node.metadata["file_name"],
            "path": node.metadata["file_path"],
            "tags": node.metadata.get("tags", ""),
            "aliases": node.metadata.get("aliases", ""),
        }
        for vector, node in zip(vectors, chunked_nodes)
    ]

    total_inserted = 0
    insert_batches = [data[i:i + MILVUS_INSERT_BATCH] for i in range(0, len(data), MILVUS_INSERT_BATCH)]
    log(f"ğŸ’¾ Milvus ì‚½ì… ì¤‘... ({len(data)} ì²­í¬, {len(insert_batches)} ë°°ì¹˜)")

    for idx, batch in enumerate(insert_batches):
        res = milvus_client.insert(collection_name=COLLECTION_NAME, data=batch)
        count = res.get("insert_count", len(batch))
        total_inserted += count
        log(f"   ë°°ì¹˜ {idx+1}/{len(insert_batches)}: {count}ê±´ ({total_inserted}/{len(data)})")

    # 5. íŠ¸ë˜í‚¹ ì—…ë°ì´íŠ¸
    update_tracking_file(processed_files)

    total_time = time.time() - t0
    print()
    log(f"ğŸ‰ ì™„ë£Œ!")
    log(f"   íŒŒì¼: {len(processed_files)}ê°œ")
    log(f"   ì²­í¬: {len(chunked_nodes)}ê°œ")
    log(f"   ì‚½ì…: {total_inserted}ê±´")
    log(f"   ì´ ì†Œìš”: {total_time:.1f}s")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="markdown-rag ë¦¬ì¸ë±ì‹± (shell ì§ì ‘ ì‹¤í–‰)")
    parser.add_argument("path", nargs="?", default=os.getcwd(),
                        help="ì¸ë±ì‹±í•  ë””ë ‰í† ë¦¬ (ê¸°ë³¸: í˜„ì¬ ë””ë ‰í† ë¦¬)")
    parser.add_argument("-f", "--force", action="store_true", help="ì „ì²´ ì¬ì¸ë±ì‹± (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)")
    parser.add_argument("--no-recursive", action="store_true", help="í•˜ìœ„ ë””ë ‰í† ë¦¬ ì œì™¸")
    args = parser.parse_args()

    asyncio.run(reindex(args.path, recursive=not args.no_recursive, force=args.force))
